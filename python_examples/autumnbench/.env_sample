OLLAMA_BASE_URL=http://host.docker.internal:11434
OPENROUTER_API_KEY="<YOUR_API_KEY_HERE>"
ANTHROPIC_API_KEY="<YOUR_ANTHROPIC_API_KEY> "
OPENAI_API_KEY="<YOUR_OPENAI_API_KEY>"
# LLM_PROVIDER=ollama
LLM_PROVIDER=openai
OLLAMA_MODEL_TEXT=qwen2.5:14b
OLLAMA_MODEL_VISION=llama3.2-vision
# MLX_MODEL=mlx-community/Qwen2.5-14B-Instruct-bf16 - Good, but too slow
MLX_MODEL=mlx-community/Qwen2.5-14B-Instruct-8bit # Faster..
MLX_SERVER_URL=http://host.docker.internal:10080
LLAMA_CPP_MODEL_PATH=/app/cache/models/llama.cpp/models/llama.cpp/models/Qwen2.5-32B-Instruct-bf16.gguf
LLAMA_CPP_N_CTX=4096
LLAMA_CPP_N_GPU_LAYERS=-1
GEMINI_API_KEY="<YOUR_GEMINI_API_KEY>"
LLM_PROVIDER_AGENT=mlx